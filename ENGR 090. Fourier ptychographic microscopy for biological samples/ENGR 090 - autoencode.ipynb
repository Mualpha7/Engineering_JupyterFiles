{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System does not have graphviz.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import layer_defs as ld\n",
    "from layer_defs import variable_on_cpu\n",
    "\n",
    "\n",
    "try:\n",
    "    import graphviz as gv\n",
    "    has_graphviz = True\n",
    "except ImportError:\n",
    "    print('System does not have graphviz.')\n",
    "    has_graphviz = False\n",
    "\n",
    "class StackedAutoEncoder(object):\n",
    "\n",
    "    def __init__(self, x_input, cnn_depth, obj_size, batch_size, \n",
    "                 training, kernel_multiplier, num_blocks, \n",
    "                 skip_interval=2, conv_activation='maxout', \n",
    "                 deconv_activation='maxout', resid=ld.residualLayer, \n",
    "                 dropout_count=0, dropout_prob=0.2, \n",
    "                 create_graph_viz=True, decay_lr_flag=False,\n",
    "                 variance_reg=1e-8, use_batch_norm=False, \n",
    "                 init_type='random', init_type_bias='trunc_norm', \n",
    "                 init_type_resid='ones'):\n",
    "\n",
    "        self.all_blocks = []\n",
    "        curr_input = x_input\n",
    "        for i in range(num_blocks):\n",
    "            with tf.variable_scope('net' + str(i)):\n",
    "                ae = AutoEncoder(curr_input, cnn_depth, obj_size, \n",
    "                                 batch_size, training, \n",
    "                                 kernel_multiplier, skip_interval=2, \n",
    "                                 conv_activation='maxout', \n",
    "                                 deconv_activation='maxout',\n",
    "                                 resid=ld.residualLayer, \n",
    "                                 dropout_count=0, dropout_prob=0.2, \n",
    "                                 create_graph_viz=True, \n",
    "                                 decay_lr_flag=False,\n",
    "                                 variance_reg=1e-8, \n",
    "                                 use_batch_norm=False, \n",
    "                                 init_type='random', \n",
    "                                 init_type_bias='trunc_norm', \n",
    "                                 init_type_resid='ones')\n",
    "                curr_input = ae.get_prediction()\n",
    "                print('curr_input')\n",
    "                print(curr_input)\n",
    "                self.all_blocks.append(ae)\n",
    "\n",
    "        print(self.all_blocks)\n",
    "\n",
    "    def get_stacked_prediction(self):\n",
    "        print('self.all_blocks[-1].get_prediction()')\n",
    "        print(self.all_blocks[-1].get_prediction())\n",
    "        \n",
    "        return self.all_blocks[-1].get_prediction()\n",
    "\n",
    "\n",
    "class AutoEncoder(object):\n",
    "    def __init__(self, x_input, cnn_depth, obj_size, batch_size, training, kernel_multiplier, skip_interval=2, conv_activation='maxout', deconv_activation='maxout',\n",
    "                    resid=ld.residualLayer, dropout_count=0, dropout_prob=0.2, create_graph_viz=True, decay_lr_flag=False,\n",
    "                    variance_reg=1e-8, use_batch_norm=False, init_type='random', init_type_bias='trunc_norm', init_type_resid='ones'):\n",
    "\n",
    "        \"\"\"Constructor for AutoEncoder class\"\"\"\n",
    "\n",
    "        self.batch_size = batch_size                                        # batch size\n",
    "        self.obj_size = obj_size                                            # size of output object\n",
    "        self.x_input = x_input                                              # x_input tensor\n",
    "        self.cnn_depth = cnn_depth                                          # number of CNN layers\n",
    "        self.skip_interval = skip_interval                                  # interval for resid (must have cnn_depth % skip_interval == 0)\n",
    "        self.num_resid = self.cnn_depth / self.skip_interval                # number of residual layers\n",
    "        self.conv_activation = conv_activation                              # conv activation/layer def\n",
    "        self.deconv_activation = deconv_activation                          # deconv activation/layer def\n",
    "        self.resid = resid                                                  # residual layer def\n",
    "        self.encode_layers, self.decode_layers = [self.x_input], []         # lists for encode and decode layers\n",
    "        self.kernel_multiplier = kernel_multiplier                          # kernel multiplier (kernel gets smaller as we get closer to center of network)\n",
    "        self.dropout_count = dropout_count                                  # number of dropout layers\n",
    "        self.dropout_prob = dropout_prob                                    # dropout probability\n",
    "        self.create_graph_viz = create_graph_viz\n",
    "        self.kernel_sizes = range(self.kernel_multiplier, (self.cnn_depth + 1) * self.kernel_multiplier, self.kernel_multiplier)\n",
    "        self.variance_reg = variance_reg\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        self.init_type = init_type\n",
    "        self.init_type_bias = init_type_bias\n",
    "        self.init_type_resid = init_type_resid\n",
    "\n",
    "        if has_graphviz:\n",
    "            g = gv.Digraph(format='svg')\n",
    "\n",
    "        dropout_encode = []\n",
    "\n",
    "        for i in range(self.cnn_depth):\n",
    "\n",
    "            with tf.variable_scope('conv' + str(i)):\n",
    "\n",
    "                kernel_index = self.cnn_depth-i-1\n",
    "                # add convolutional layer\n",
    "                initializer = ld.getConvInitializer(self.kernel_sizes[kernel_index], self.kernel_sizes[kernel_index], init_type=self.init_type)\n",
    "\n",
    "                # add dropout layer\n",
    "                if i >= self.cnn_depth - self.dropout_count:\n",
    "                    dropout = ld.dropoutLayer(self.encode_layers[-1], rate=self.dropout_prob, training=training)\n",
    "\n",
    "                    if has_graphviz:\n",
    "                        self.__add_node_and_edge(g, self.encode_layers[-1], dropout)\n",
    "\n",
    "                    self.encode_layers.append(dropout)\n",
    "\n",
    "                    dropout_encode.append(i)\n",
    "\n",
    "                if self.conv_activation == 'relu':\n",
    "                    cnn_layer = ld.conv_layer(self.encode_layers[-1],\n",
    "                                              variable_on_cpu('conv_kernel' + str(i), initializer, tf.float32),\n",
    "                                              name_index=i,\n",
    "                                              init_type=self.init_type_bias)\n",
    "\n",
    "                    if self.use_batch_norm:\n",
    "                        cnn_layer = ld.batch_norm_layer(cnn_layer, self.variance_reg)\n",
    "\n",
    "                    cnn_layer = tf.nn.relu(cnn_layer)\n",
    "\n",
    "\n",
    "                elif self.conv_activation == 'maxout':\n",
    "\n",
    "                    with tf.variable_scope('maxout_layer1'):\n",
    "\n",
    "                        cnn_layer1 = ld.conv_layer(self.encode_layers[-1],\n",
    "                                                   variable_on_cpu('conv_kernel1' + str(i), initializer, tf.float32),\n",
    "                                                   name_index=i,\n",
    "                                                   init_type=self.init_type_bias)\n",
    "\n",
    "                        if self.use_batch_norm:\n",
    "                            cnn_layer1 = ld.batch_norm_layer(cnn_layer1, self.variance_reg)\n",
    "\n",
    "                    with tf.variable_scope('maxout_layer2'):\n",
    "\n",
    "                        cnn_layer2 = ld.conv_layer(self.encode_layers[-1],\n",
    "                                                   variable_on_cpu('conv_kernel2' + str(i) ,initializer, tf.float32),\n",
    "                                                   name_index=i,\n",
    "                                                   init_type=self.init_type_bias)\n",
    "\n",
    "                        if self.use_batch_norm:\n",
    "                            cnn_layer2 = ld.batch_norm_layer(cnn_layer2, self.variance_reg)\n",
    "\n",
    "                    cnn_layer = ld.maxout(cnn_layer1, cnn_layer2)\n",
    "\n",
    "                if has_graphviz:\n",
    "                    self.__add_node_and_edge(g, self.encode_layers[-1], cnn_layer)\n",
    "\n",
    "                self.encode_layers.append(cnn_layer)\n",
    "\n",
    "\n",
    "\n",
    "        with tf.variable_scope('deconv0'):\n",
    "            # create first deconv layer joined to last conv layer\n",
    "            deconv_initializer = ld.getConvInitializer(self.kernel_sizes[0], self.kernel_sizes[0], init_type=self.init_type)\n",
    "\n",
    "            if self.deconv_activation == 'relu':\n",
    "\n",
    "                first_deconv = ld.deconv_layer(self.encode_layers[-1],\n",
    "                                               variable_on_cpu('deconv_kernel0',deconv_initializer, tf.float32),\n",
    "                                               self.obj_size, self.batch_size, name_index=0,\n",
    "                                               init_type=self.init_type_bias)\n",
    "\n",
    "\n",
    "                if self.use_batch_norm:\n",
    "                    first_deconv = ld.batch_norm_layer(first_deconv, self.variance_reg)\n",
    "\n",
    "                first_deconv = tf.nn.relu(first_deconv)\n",
    "\n",
    "            elif self.deconv_activation == 'maxout':\n",
    "                with tf.variable_scope('maxout_layer1'):\n",
    "                    first_deconv1 = ld.deconv_layer(self.encode_layers[-1],\n",
    "                                                    variable_on_cpu('deconv_kernel0_1',deconv_initializer, tf.float32),\n",
    "                                                    self.obj_size, self.batch_size, name_index=0,\n",
    "                                                    init_type=self.init_type_bias)\n",
    "\n",
    "                    if self.use_batch_norm:\n",
    "                        first_deconv1 = ld.batch_norm_layer(first_deconv1, self.variance_reg)\n",
    "\n",
    "                with tf.variable_scope('maxout_layer2'):\n",
    "                    first_deconv2 = ld.deconv_layer(self.encode_layers[-1],\n",
    "                                                    variable_on_cpu('deconv_kernel0_2',deconv_initializer, tf.float32),\n",
    "                                                    self.obj_size, self.batch_size, name_index=0,\n",
    "                                                    init_type=self.init_type_bias)\n",
    "\n",
    "                    if self.use_batch_norm:\n",
    "                        first_deconv2 = ld.batch_norm_layer(first_deconv2, self.variance_reg)\n",
    "\n",
    "\n",
    "                first_deconv = ld.maxout(first_deconv1, first_deconv2)\n",
    "\n",
    "\n",
    "            if has_graphviz:\n",
    "                self.__add_node_and_edge(g, self.encode_layers[-1], first_deconv)\n",
    "\n",
    "            self.decode_layers.append(first_deconv)\n",
    "\n",
    "\n",
    "        num_resid = 0\n",
    "        num_dropout_decode = 0\n",
    "\n",
    "        # loop to create deconvolutional and residual layers\n",
    "        # possible layer types: deconvolutional, residual, dropout\n",
    "\n",
    "        for i in range(1,self.cnn_depth):\n",
    "\n",
    "            with tf.variable_scope('deconv' + str(i)):\n",
    "                # add convolutional layer\n",
    "                deconv_initializer = ld.getConvInitializer(self.kernel_sizes[i], self.kernel_sizes[i], init_type=self.init_type)\n",
    "\n",
    "                # add dropout layer\n",
    "\n",
    "                if i >= self.cnn_depth - self.dropout_count:\n",
    "                    dropout = ld.dropoutLayer(self.decode_layers[-1], rate=self.dropout_prob, training=training)\n",
    "\n",
    "                    if has_graphviz:\n",
    "                        self.__add_node_and_edge(g, self.decode_layers[-1], dropout)\n",
    "\n",
    "                    self.decode_layers.append(dropout)\n",
    "                    num_dropout_decode += 1\n",
    "\n",
    "\n",
    "                if self.deconv_activation == 'relu':\n",
    "                    deconv_layer = ld.deconv_layer(self.decode_layers[-1],\n",
    "                                                   variable_on_cpu('deconv_kernel' + str(i),deconv_initializer, tf.float32),\n",
    "                                                   self.obj_size, self.batch_size, name_index=i,\n",
    "                                                   init_type=self.init_type_bias)\n",
    "\n",
    "                    if self.use_batch_norm:\n",
    "                        deconv_layer = ld.batch_norm_layer(deconv_layer, self.variance_reg)\n",
    "\n",
    "                    deconv_layer = tf.nn.relu(deconv_layer)\n",
    "\n",
    "                elif self.deconv_activation == 'maxout':\n",
    "                    with tf.variable_scope('maxout_layer1'):\n",
    "                        deconv_layer1 = ld.deconv_layer(self.decode_layers[-1],\n",
    "                                                        variable_on_cpu('deconv_kernel1' + str(i),deconv_initializer, tf.float32),\n",
    "                                                        self.obj_size, self.batch_size, name_index=i,\n",
    "                                                        init_type=self.init_type_bias)\n",
    "\n",
    "                        if self.use_batch_norm:\n",
    "                            deconv_layer1 = ld.batch_norm_layer(deconv_layer1, self.variance_reg)\n",
    "\n",
    "                    with tf.variable_scope('maxout_layer2'):\n",
    "                        deconv_layer2 = ld.deconv_layer(self.decode_layers[-1],\n",
    "                                                        variable_on_cpu('deconv_kernel2' + str(i),deconv_initializer, tf.float32),\n",
    "                                                        self.obj_size, self.batch_size, name_index=i,\n",
    "                                                        init_type=self.init_type_bias)\n",
    "\n",
    "                        if self.use_batch_norm:\n",
    "                            deconv_layer2 = ld.batch_norm_layer(deconv_layer2, self.variance_reg)\n",
    "\n",
    "                    deconv_layer = ld.maxout(deconv_layer1, deconv_layer2)\n",
    "\n",
    "                if has_graphviz:\n",
    "                    self.__add_node_and_edge(g, self.decode_layers[-1], deconv_layer)\n",
    "\n",
    "                self.decode_layers.append(deconv_layer)\n",
    "\n",
    "                # add residual layer if we reach a skip interval\n",
    "                if (len(self.decode_layers) - num_resid - num_dropout_decode) % self.skip_interval == 0 \\\n",
    "                    and (len(self.decode_layers) - num_resid - num_dropout_decode) != 0 :\n",
    "\n",
    "                    layer_i = self.cnn_depth - i - 1\n",
    "                    num_dropout_encode = 0\n",
    "                    for m in range(len(dropout_encode)):\n",
    "                        if dropout_encode[m] < layer_i:\n",
    "                            num_dropout_encode += 1\n",
    "\n",
    "\n",
    "                    resid = self.resid(self.encode_layers[self.cnn_depth - i - 1 + num_dropout_encode], deconv_layer, \\\n",
    "                                       name_index=num_resid, init_type=self.init_type_resid)\n",
    "#                    resid = self.resid(self.encode_layers[self.cnn_depth - i - 1 + num_dropout_encode], deconv_layer, name_index=num_resid)\n",
    "\n",
    "\n",
    "                    if has_graphviz:\n",
    "                        self.__add_node_and_edge(g, self.encode_layers[self.cnn_depth-i-1+num_dropout_encode], resid)\n",
    "                        self.__add_node_and_edge(g, deconv_layer, resid)\n",
    "\n",
    "                    self.decode_layers.append(resid)\n",
    "                    num_resid += 1\n",
    "\n",
    "\n",
    "\n",
    "        self.all_layers = self.encode_layers + self.decode_layers\n",
    "\n",
    "        if has_graphviz:\n",
    "            if self.create_graph_viz:\n",
    "                g.render('img/g2')\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Autoencoder-Network-\" + self.x_input.name()\n",
    "\n",
    "    def __format_graph_name(self, t):\n",
    "        return t.name[:len(t.name)-2]\n",
    "\n",
    "    def __format_graph_edge(self, t1, t2):\n",
    "        return self.__format_graph_name(t1), self.__format_graph_name(t2)\n",
    "\n",
    "    def __add_node_and_edge(self, g, t1, t2):\n",
    "        new = self.__format_graph_name(t2)\n",
    "        g.node(new)\n",
    "        g.edge(self.__format_graph_name(t1), new)\n",
    "\n",
    "    def get_prediction(self):\n",
    "        return self.all_layers[-1]\n",
    "\n",
    "    # def train_ae(self):\n",
    "    #     loss = getLoss(x_train_node, y_predict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
