{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def variable_on_cpu(name, initial_value, dtype, trainable=True, constraint=None, shape=None):\n",
    "    \"\"\"Helper to create a Variable stored on CPU memory.\"\"\"\n",
    "\n",
    "    with tf.device('/cpu:0'):\n",
    "        if shape == None:\n",
    "            var = tf.get_variable(name, dtype=dtype, initializer=initial_value, trainable=trainable, constraint=constraint)\n",
    "        else:\n",
    "            var = tf.get_variable(name, dtype=dtype, initializer=initial_value, trainable=trainable, constraint=constraint, shape=shape)\n",
    "    return var\n",
    "\n",
    "#def convLayer(layer_input, sizes, kernel_init, padding=\"same\", activation=tf.nn.relu):\n",
    "#    \"\"\"Creates a convolutional layer.\n",
    "#    Sizes is list with [filters, kernel_x, kernel_y]\"\"\"\n",
    "#\n",
    "#    conv = tf.layers.conv2d(\n",
    "#        inputs=layer_input,\n",
    "#        filters=sizes[0],\n",
    "#        kernel_size=sizes[1:],\n",
    "#        padding=padding,\n",
    "#        activation=activation,\n",
    "#        kernel_initializer=kernel_init\n",
    "#    )\n",
    "#\n",
    "#    return conv\n",
    "\n",
    "def conv_layer(x_in, w, strides=[1,1,1,1], padding=\"SAME\", name=\"Conv_\", name_index=0, init_type=\"trunc_norm\"):\n",
    "    b = variable_on_cpu('bias', getConvInitializer(1, 1, init_type=init_type), tf.float32)\n",
    "    return tf.nn.conv2d(x_in, w, strides=strides, padding=padding, name=name+\"-\"+str(name_index)) + b\n",
    "\n",
    "def deconv_layer(x_in, w, obj_size, batch_size, strides=[1,1,1,1], padding=\"SAME\", name=\"DeConv_\", name_index=0, num_channels=1, init_type=\"trunc_norm\"):\n",
    "    b = variable_on_cpu('bias', getConvInitializer(1, 1, init_type=init_type), tf.float32)\n",
    "    return tf.nn.conv2d_transpose(x_in, w, [batch_size, obj_size, obj_size, num_channels], strides, padding=padding, name=name+\"-\"+str(name_index)) + b\n",
    "\n",
    "#def conv_relu(x_in, w, strides=[1,1,1,1], padding=\"SAME\", name=\"Conv_\", name_index=0):\n",
    "#    return tf.nn.relu(tf.nn.conv2d(x_in, w, strides=strides, padding=padding), name=name+\"-\"+str(name_index))\n",
    "\n",
    "def maxout(conv_layer1, conv_layer2):\n",
    "    return tf.maximum(conv_layer1, conv_layer2)\n",
    "#    return tf.where(tf.abs(conv_layer1)>tf.abs(conv_layer2),conv_layer1,conv_layer2)\n",
    "#    return tf.minimum(conv_layer1, conv_layer2)\n",
    "\n",
    "#def conv_maxout(x_in, w, strides=[1,1,1,1], padding=\"SAME\", name=\"Conv_Maxout\", num_units=1, name_index=0):\n",
    "#    return tf.contrib.layers.maxout(tf.nn.conv2d(x_in, w, strides=strides, padding=padding), num_units, name=name+\"-\"+str(name_index))\n",
    "\n",
    "#def deconv_relu(x_in, w, obj_size, batch_size, strides=[1,1,1,1], padding=\"SAME\", name=\"DeConv_ReLU\", name_index=0, num_channels=1):\n",
    "#    print('w: ', w)\n",
    "#    return tf.nn.relu(tf.nn.conv2d_transpose(x_in, w, [batch_size, obj_size, obj_size, num_channels], strides, padding=padding), name=name+\"-\"+str(name_index))\n",
    "\n",
    "#def deconv_maxout(x_in, w, obj_size, batch_size, strides=[1,1,1,1], padding=\"SAME\", name=\"DeConv_Maxout\", num_units=1, name_index=0, num_channels=1):\n",
    "#    return tf.contrib.layers.maxout(tf.nn.conv2d_transpose(x_in, w, [batch_size, obj_size, obj_size, num_channels], strides, padding=padding), num_units, name=name+\"-\"+str(name_index))\n",
    "\n",
    "#def deconv_maxout_upsample(x_in, w, obj_size, strides=[1,1,1,1], padding=\"SAME\", size=1, name=\"DeConv_Maxout_Upsample\", name_index=0):\n",
    "#    upsampled = tf.image.resize_images(x_in, size=(obj_size, size), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "#    return tf.contrib.layers.maxout(tf.nn.conv2d(upsampled, w, strides=strides, padding=padding), 1, name=name+\"-\"+str(name_index))\n",
    "\n",
    "def getConvInitializer(width, height, num_channels=1, filter_depth=1, init_type=\"trunc_norm\"):\n",
    "    \"\"\" \"\"\"\n",
    "\n",
    "    if init_type == \"delta\":\n",
    "        init = np.zeros((width, height, num_channels, filter_depth),dtype=np.float32)\n",
    "        init[width/2][height/2][0][0] = 1\n",
    "        return init\n",
    "\n",
    "    if init_type == \"negative_delta\":\n",
    "        init = np.zeros((width, height, num_channels, filter_depth),dtype=np.float32)\n",
    "        init[width/2][height/2][0][0] = -10000\n",
    "        return init\n",
    "\n",
    "    elif init_type == \"zeros\":\n",
    "        init = np.zeros((width, height, num_channels, filter_depth),dtype=np.float32)\n",
    "        return init\n",
    "       \n",
    "    elif init_type == \"big_delta\":\n",
    "        inner_size = (10, 10)\n",
    "        pad_size = ((width-inner_size[0])/2, (height-inner_size[1])/2)\n",
    "\n",
    "        mat = tf.constant(np.ones([inner_size[0], inner_size[1]]), dtype=tf.float32)\n",
    "\n",
    "        if width % 2 == 0:\n",
    "            paddings = tf.constant([[pad_size[1], pad_size[1]], [pad_size[0], pad_size[0]]])\n",
    "        else:\n",
    "            paddings = tf.constant([[pad_size[1], pad_size[1]+1], [pad_size[0], pad_size[0]+1]])\n",
    "\n",
    "        p = tf.pad(mat, paddings, \"CONSTANT\")\n",
    "\n",
    "        return tf.reshape(p, (width, height, num_channels, filter_depth))\n",
    "\n",
    "    elif init_type == \"random\":\n",
    "        return tf.cast(np.random.rand(width, height, num_channels, filter_depth)-0.5, tf.float32)\n",
    "\n",
    "    elif init_type == \"trunc_norm\":\n",
    "        return tf.truncated_normal([width, height, num_channels, filter_depth], stddev=0.1)\n",
    "\n",
    "    elif init_type == \"gauss\":\n",
    "        sigma = 0.5\n",
    "        halfwidth_x = np.arange(-width/2, width/2)\n",
    "        halfwidth_y = np.arange(-height/2, height/2)\n",
    "        xx, yy = np.meshgrid(halfwidth_x, halfwidth_y)\n",
    "        gauss = np.reshape(np.exp(-1/float((2*sigma**2)) * (xx**2+yy**2)), (width, height, num_channels, filter_depth))\n",
    "        return tf.cast(gauss, tf.float32)\n",
    "\n",
    "    elif init_type == \"rand_unif\":\n",
    "        return tf.random_uniform([width, height, num_channels, filter_depth])\n",
    "\n",
    "    elif init_type == \"ones\":\n",
    "        return tf.ones([width, height, num_channels, filter_depth])\n",
    "\n",
    "    elif init_type == \"fill\":\n",
    "        fill_value = 100.\n",
    "        filled = tf.fill([width, height], fill_value)\n",
    "        return tf.reshape(filled, (width, height, num_channels, filter_depth))\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Please provide a valid init_type argument.\")\n",
    "\n",
    "\n",
    "\n",
    "def residualLayer(input1, skipped_input, name=\"Resid_ReLU\", name_index=0, init_type=\"trunc_norm\"): #activation=tf.nn.relu\n",
    "    \"\"\"Returns residual layer that adds inputs and passes them through ReLU\"\"\"\n",
    "\n",
    "    resid_multiplier = variable_on_cpu('resid_multiplier', getConvInitializer(1, 1, init_type=init_type), tf.float32)\n",
    "    added = tf.add(resid_multiplier*input1, skipped_input)\n",
    "    \n",
    "#    return activation(added, name=name+\"-\"+str(name_index))\n",
    "    \n",
    "    return added\n",
    "\n",
    "def dropoutLayer(input_, rate=0.7, training=True, name=\"Dropout\"):\n",
    "    \"\"\" Wrapper for dropout layer \"\"\"\n",
    "\n",
    "    return tf.layers.dropout(input_, rate=rate, training=training, name=name)\n",
    "\n",
    "def batch_norm_layer(conv_layer, variance_reg):\n",
    "    \"\"\" Wrapper for batch norm layer \"\"\"\n",
    "\n",
    "    # Calculate batch mean and variance\n",
    "    batch_mean, batch_var = tf.nn.moments(conv_layer,[0,1,2,3])\n",
    "\n",
    "    # Apply the initial batch normalizing transform\n",
    "    conv_layer = (conv_layer - batch_mean) / tf.sqrt(batch_var + tf.constant(variance_reg,tf.float32))\n",
    "\n",
    "    # Create two new parameters, scale and beta (shift)\n",
    "    scale = variable_on_cpu('scale', 1.0, tf.float32)\n",
    "    beta = variable_on_cpu('beta', 0.0, tf.float32)\n",
    "\n",
    "    # Scale and shift to obtain the final output of the batch normalization\n",
    "\n",
    "    conv_layer = scale * conv_layer + beta\n",
    "\n",
    "    return conv_layer\n",
    "\n",
    "def create_conv_layer(kernel_length, input_layer, input_channels, output_channels):\n",
    "    # input_channels = int(input_layer.shape[3])\n",
    "    # output_channels = Nz\n",
    "    kernel = variable_on_cpu('kernel', tf.truncated_normal([kernel_length,kernel_length,input_channels,output_channels], \\\n",
    "                                                            mean=0, stddev=0.1, dtype=tf.float32), tf.float32)\n",
    "    biases = variable_on_cpu('biases', tf.truncated_normal([output_channels], mean=0, stddev=0.1, dtype=tf.float32), tf.float32)\n",
    "    conv_layer = tf.nn.conv2d(input_layer, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, biases)\n",
    "    return conv_layer\n",
    "\n",
    "def create_residual_layer_maxout(kernel_length_vec,input_layer,use_batch_norm,input_channels,output_channels,variance_reg,dropout_prob,training):\n",
    "    # kernel_length_vec contains 2 lengths\n",
    "\n",
    "    input_layer = tf.layers.dropout(inputs=input_layer, rate=dropout_prob, training=training) #rate is percentage that is dropped\n",
    "\n",
    "    with tf.variable_scope(\"conv1\"):\n",
    "        conv1 = create_conv_layer(kernel_length_vec[0],input_layer,input_channels,output_channels)\n",
    "\n",
    "        if use_batch_norm:\n",
    "            conv1 = batch_norm_layer(conv1,variance_reg)\n",
    "\n",
    "    with tf.variable_scope(\"conv2\"):\n",
    "        conv2 = create_conv_layer(kernel_length_vec[0],input_layer,input_channels,output_channels)\n",
    "        if use_batch_norm:\n",
    "            conv2 = batch_norm_layer(conv2,variance_reg)\n",
    "\n",
    "        maxout1 = tf.maximum(conv1,conv2)\n",
    "\n",
    "        maxout1 = tf.layers.dropout(inputs=maxout1, rate=dropout_prob, training=training) #rate is percentage that is dropped\n",
    "        \n",
    "    with tf.variable_scope(\"conv3\"):\n",
    "        conv3 = create_conv_layer(kernel_length_vec[1],maxout1,output_channels,output_channels)\n",
    "        added_layer1 = conv3 + input_layer\n",
    "        \n",
    "        if use_batch_norm:\n",
    "            added_layer1 = batch_norm_layer(added_layer1,variance_reg)\n",
    "            \n",
    "    with tf.variable_scope(\"conv4\"):\n",
    "        conv4 = create_conv_layer(kernel_length_vec[1],maxout1,output_channels,output_channels)\n",
    "        added_layer2 = conv4 + input_layer\n",
    "        \n",
    "        if use_batch_norm:\n",
    "            added_layer2 = batch_norm_layer(added_layer2,variance_reg)\n",
    "            \n",
    "        res_layer = tf.maximum(added_layer1,added_layer2)\n",
    "\n",
    "    return res_layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
